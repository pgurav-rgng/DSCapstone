clean<-tm_map(clean,content_transformer(gsub),pattern="doesn't",replace="does not")
clean<-tm_map(clean,content_transformer(gsub),pattern="haven't",replace="have not")
clean<-tm_map(clean,content_transformer(gsub),pattern="n't",replace="not")
clean<-tm_map(clean,content_transformer(gsub),pattern="'ll",replace="will")
clean<-tm_map(clean,content_transformer(gsub),pattern="'m",replace="am")
clean<-tm_map(clean,content_transformer(gsub),pattern="'d",replace="would")
clean<-tm_map(clean,content_transformer(gsub),pattern="'s",replace="")
clean<-tm_map(clean,content_transformer(gsub),pattern="let's",replace="let us")
clean<-tm_map(clean,removeNumbers)
clean<-tm_map(clean,content_transformer(gsub),pattern="\\W",replace=" ")
clean<- tm_map(clean, content_transformer(gsub), pattern = "-|\\/|\\.\\.\\.", replacement = " ")
clean <- tm_map(clean, content_transformer(gsub), pattern = "(http\\:\\/\\/|https\\:\\/\\/)?(www.)?[a-z\\.]*?[a-z]*\\.(org|com|gov|net|edu)", replacement = " ")
clean <- tm_map(clean, content_transformer(gsub), pattern = ".html|.htm|.aspx", replacement = " ")
clean <- tm_map(clean, content_transformer(gsub), pattern = "\177|\032", replacement = " ")
clean <- tm_map(clean, content_transformer(gsub), pattern = "[A-Za-z0-9]*\\@[a-z]*\\.([a-zA-Z]{3}|[a-zA-Z]{2})", replacement = " ")
clean <- tm_map(clean, content_transformer(str_replace_all), pattern = "(\\w)\\1{2,}", replacement = "\1")
clean<-tm_map(clean,removePunctuation)
clean<-tm_map(clean,stripWhitespace)
clean<-tm_map(clean, removeWords, stopwords("english"))
#Remove words longer than 16 characters
clean <- tm_map(clean, content_transformer(gsub), pattern = "\\b\\w{16,}", replacement = "")
clean <- tm_map(clean, content_transformer(gsub), pattern = "#", replacement = " ")
clean <- tm_map(clean, content_transformer(gsub), pattern = "\\$", replacement = " ")
clean <- tm_map(clean, content_transformer(gsub), pattern = "<u+0099>", replacement = " ")
clean <- tm_map(clean, content_transformer(gsub), pattern = "&", replacement = " ")
clean <- tm_map(clean, content_transformer(gsub), pattern = "\\*", replacement = " ")
clean <- tm_map(clean, content_transformer(gsub), pattern = "\\[", replacement = " ")
clean <- tm_map(clean, content_transformer(gsub), pattern = "\\]", replacement = " ")
clean <- tm_map(clean, content_transformer(gsub), pattern = "aa", replacement = " a")
clean <- tm_map(clean, stemDocument,language = ("english"))
knitr::kable(data.frame(Dataset=c("en_US.blogs.txt","en_US.news.txt","en_US.twitter.txt"), FileSize=paste0(round(filesizes/10^6,0)," GB"),Lines=paste0(round(linecounts,1)), TotalWords=paste0(round(totalwords/10^3,1),"K")))
trigram
library(wordcloud2)
library(wordcloud)
uni.freq[1:100,]
head(uni.freq[1:100,])
uni
knitr::opts_chunk$set(echo = TRUE)
uni <- findFreqTerms(uni.spa,lowfreq=500)
install.packages("tm")
install.packages("RColorBrewer")
library("SnowballC")
display.brewer.pal(n = 8, name = 'RdBu')
library(RColorBrewer)
display.brewer.pal(n = 8, name = 'RdBu')
display.brewer.pal(n = 8, name = 'Blues')
url <- "http://cran.us.r-project.org/src/contrib/Archive/cldr/cldr_1.1.0.tar.gz"
pkgFile<-"cldr_1.1.0.tar.gz"
download.file(url = url, destfile = pkgFile)
install.packages(pkgs=pkgFile, type="source", repos=NULL)
unlink(pkgFile)
library(cldr)
demo(cldr)
display.brewer.pal(n = 4, name = 'Blues')
display.brewer.pal(n = 8, name = "Blues"))[1:4]
display.brewer.pal(n = 8, name = "Blues")[1:4]
knitr::opts_chunk$set(echo = TRUE)
getCumulativeFreq <- function(tokens) {
return(cumsum(tokens$frequency))
}
getCoverage <- function(cumdist, total, target) {
targetfreq <- totalfreq * target
return(which(cumdist >= targetfreq)[1])
}
# Cumulative distribution of word frequencies
cumulativedist <- getCumulativeFreq(uni.freq)
getCumulativeFreq <- function(tokens) {
return(cumsum(tokens$frequency))
}
getCoverage <- function(cumdist, total, target) {
targetfreq <- totalfreq * target
return(which(cumdist >= targetfreq)[1])
}
# Cumulative distribution of word frequencies
cumulativedist <- getCumulativeFreq(uni.freq)
install.packages("frequency")
library(frequency)
install.packages("qdap")
install.packages("Rgraphviz")
knitr::opts_chunk$set(echo = TRUE)
uni.spa <- removeSparseTerms(unigram, 0.99)
library(tm)
library(R.utils)
library(SDraw)
library(textmineR)
library(RWeka)
library(dplyr)
library(data.table)
library(wordcloud2)
library(wordcloud)
library(ggplot2)
library(stringr)
library(frequency)
uni.spa <- removeSparseTerms(unigram, 0.99)
str(unigram)
str(unigram)
uni.spa <- removeSparseTerms(unigram, 0.99)
uni.spa <- removeSparseTerms(unigram, 0.99)
uni.spa <- removeSparseTerms(unigram, 0.99)
unlink('datascience/capstone/nlp1_cache', recursive = TRUE)
unlink('datascience/capstone/nlp1_cache', recursive = TRUE)
unlink('Textminproj_cache', recursive = TRUE)
install.packages("tidytext")
unlink('Textminproj_cache', recursive = TRUE)
unlink('Textminproj_cache', recursive = TRUE)
unlink('Textminproj_cache', recursive = TRUE)
knitr::opts_chunk$set(echo = TRUE)
library(tm)
library(R.utils)
library(RWeka)
library(dplyr)
library(data.table)
library(wordcloud)
library(ggplot2)
blogfile<- "C:/Users/pg/Documents/datascience/capstone/final/en_US/en_US.blogs.txt"
newsfile<- "C:/Users/pg/Documents/datascience/capstone/final/en_US/en_US.news.txt"
twitfile<- "C:/Users/pg/Documents/datascience/capstone/final/en_US/en_US.twitter.txt"
badwords<- "C:/Users/pg/Documents/datascience/capstone/final/en_US/badwords.txt"
size.blogs=file.info(blogfile)$size
size.news=file.info(newsfile)$size
size.twitter=file.info(twitfile)$size
# Lines and word counts of blogs
blogs<-readLines(con <- file(blogfile,"r"),skipNul=TRUE)
close(con)
line.blogs=length(blogs)
word.blogs=summary(nchar(blogs))
total.word.blogs=sum(nchar(blogs))
# Lines and word counts of news
news<-readLines(con <- file(newsfile,"r"),skipNul=TRUE)
close(con)
line.news=length(news)
word.news=summary(nchar(news))
total.word.news=sum(nchar(news))
# Lines and word counts of twitter
twitter<-readLines(con <- file(twitfile,"r"),skipNul=TRUE)
close(con)
line.twitter=length(twitter)
word.twitter=summary(nchar(twitter))
total.word.twitter=sum(nchar(twitter))
# Generate data table
filesizes=c(size.blogs,size.news,size.twitter)
linecounts=c(line.blogs,line.news,line.twitter)
totalwords=c(total.word.blogs,total.word.news,total.word.twitter)
minwords=c(min(word.blogs),min(word.news),min(word.twitter))
maxwords=c(max(word.blogs),max(word.news),max(word.twitter))
knitr::kable(data.frame(Dataset=c("en_US.blogs.txt","en_US.news.txt","en_US.twitter.txt"),
FileSize=paste0(round(filesizes/10^6,0)," MB"),
Lines=paste0(round(linecounts,1)), TotalWords=paste0(round(totalwords/10^3,1),"K")))
badwordslines<-readLines(con <- file(badwords,"r"),skipNul=TRUE)
close(con)
.contract <- function(contract) {
contract <- gsub("won't", "will not", contract)
contract <- gsub("n't", " not", contract)
contract <- gsub("'ll", " will", contract)
contract <- gsub("'re", " are", contract)
contract <- gsub("'ve", " have", contract)
contract <- gsub("'m", " am", contract)
contract <- gsub("'s", "", contract)
return(contract)
}
# sample and process corpora
SPC<-function(x) {
x<-VCorpus(VectorSource(x))
x<-tm_map(x,content_transformer(gsub),pattern="won't",replace="will not")
x<-tm_map(x,content_transformer(gsub),pattern="n't",replace=" not")
x<-tm_map(x,content_transformer(gsub),pattern="'ll",replace=" will")
x<-tm_map(x,content_transformer(gsub),pattern="'re",replace=" are")
x<-tm_map(x,content_transformer(gsub),pattern="'ve",replace=" have")
x<-tm_map(x,content_transformer(gsub),pattern="'m",replace=" am")
x<-tm_map(x,content_transformer(gsub),pattern="'s",replace="")
x<-tm_map(x,removeNumbers)
x<-tm_map(x,removeWords, stopwords("english"))
x<-tm_map(x,removeWords, badwordslines)
x<-tm_map(x,stripWhitespace)
x<-tm_map(x, content_transformer(tolower))
x<-tm_map(x, removePunctuation,preserve_intra_word_dashes = TRUE)
}
sampleSize <- 2000
sample_blogs <- readLines(blogfile, n = sampleSize)
sample_news <- readLines(newsfile, n = sampleSize)
sample_twitter <- readLines(twitfile, n = sampleSize)
samples <- c(sample_blogs, sample_news, sample_twitter)
# find indices of words with non-ASCII characters, remove words with non-ASCII characters
dat <- grep("corpus", iconv(samples, "latin1", "ASCII", sub="corpus"))
# subset original vector of words to exclude words with non-ASCII char
samples <- samples[-dat]
# convert vector back to a string
samples <- paste(samples, collapse = ", ")
samples_a<-SPC(samples);
UniToken<-function(x)NGramTokenizer(x, Weka_control(min = 1, max = 1))
BiToken<-function(x)NGramTokenizer(x, Weka_control(min = 2, max = 2))
TriToken<-function(x)NGramTokenizer(x, Weka_control(min = 3, max = 3))
myDtm <- TermDocumentMatrix(samples_a, control = list(minWordLength = 1))
#findFreqTerms(myDtm, lowfreq=100)
uni.spa <- TermDocumentMatrix(samples_a, control = list(tokenize = UniToken))
uni <- findFreqTerms(uni.spa)
uni.sort <- sort(rowSums(as.matrix(uni.spa[uni,])),decreasing=TRUE)
uni.freq <- data.frame(word=names(uni.sort),frequency=uni.sort)
ggplot(uni.freq[1:20,], aes(factor(word, levels=unique(word)),frequency, fill=frequency)) +
geom_bar(stat = 'identity') +
theme(axis.text.x=element_text(angle=90)) +
labs(title="Unigram") +
xlab('Top20 Words') +
ylab('Frequency')
m <- as.matrix(myDtm)
# calculate the frequency of words
v <- sort(rowSums(m), decreasing=TRUE)
myNames <- names(v)
head(myNames)
d <- data.frame(word=myNames, freq=v)
wordcloud(d$word, d$freq, max.words =100,colors=brewer.pal(8, "Dark2"))
# bigram wordcloud code
tdm.bigram <- TermDocumentMatrix(samples_a, control = list(tokenize = BiToken))
# Try removing sparse terms at a few different levels
tdm99.bigram  <- removeSparseTerms(tdm.bigram, 0.99)
bi<- findFreqTerms(tdm99.bigram)
bi.sort <- sort(rowSums(as.matrix(tdm99.bigram[bi,])),decreasing=TRUE)
bi.freq <- data.frame(word=names(bi.sort),frequency=bi.sort)
ggplot(bi.freq[1:20,], aes(factor(word, levels=unique(word)),frequency, fill=frequency)) +
geom_bar(stat = 'identity') +
theme(axis.text.x=element_text(angle=90)) +
labs(title="Bigram") +
xlab('Top20 Words') +
ylab('Frequency')
m2 = as.matrix(tdm99.bigram)
v2 = sort(rowSums(m2),decreasing=TRUE)
d2 = data.frame(word = names(v2),freq=v2)
#str(d2)
# Create the word cloud
# pal = brewer.pal(9,"BuPu")
# wordcloud(words = d2$word,
#           freq = d2$freq, scale = c(3,.8),
#           random.order = F,
#           relative_scaling= 0, prefer_horizontal = 0.6,
#           rot.per=0.35,
#           colors = pal,
#           max.words =40 )
wordcloud(d2$word, d2$freq, min.freq = 1, max.words =30,scale = c(3.5,0.2), colors=brewer.pal(8, "Dark2"))
# trigram wordcloud code
tdm.trigram <- TermDocumentMatrix(samples_a, control = list(tokenize = TriToken))
# Try removing sparse terms at a few different levels
tdm999.trigram  <- removeSparseTerms(tdm.trigram, 0.999)
tri<- findFreqTerms(tdm999.trigram)
tri.sort <- sort(rowSums(as.matrix(tdm999.trigram[tri,])),decreasing=TRUE)
tri.freq <- data.frame(word=names(tri.sort),frequency=tri.sort)
ggplot(tri.freq[1:20,], aes(factor(word, levels=unique(word)),frequency, fill=frequency)) +
geom_bar(stat = 'identity') +
theme(axis.text.x=element_text(angle=90)) +
labs(title="Trigram") +
xlab('Top20 Words') +
ylab('Frequency')
m3 = as.matrix(tdm999.trigram)
v3 = sort(rowSums(m3),decreasing=TRUE)
d3 = data.frame(word = names(v3),freq=v3)
wordcloud(d3$word, d3$freq,min.freq = 1, max.words =30,scale = c(3.0,.25),colors=brewer.pal(8, "Dark2"))
unigrams<-function(x)
{tdm <- TermDocumentMatrix(x, control = list(tokenize = UniToken))
fm <- rowSums(as.matrix(tdm))
ngram<-data.frame(ngram=names(fm),freq=fm)
ngram<-ngram[order(-ngram$freq),]
}
full_a1<-unigrams(samples_a)
wordcoverage<-function(x,wordcover) #x is the unigram output sorted by frequency, y is the percent word coverage
{nwords<-0 # initial counter
coverage<-wordcover*sum(x$freq) # number of words to hit coverage
for (i in 1:nrow(x))
{if (nwords >= coverage) {return (i)}
nwords<-nwords+x$freq[i]
}}
dictionary.size.to.coverage.uni <- cumsum(full_a1$freq * 100 / sum(full_a1$freq))
fiftypercentcoverage<-wordcoverage(full_a1,0.5)
fiftypercentcoverage<-wordcoverage(full_a1,0.5)
fiftypercentcoverage
ninetypercentcoverage<-wordcoverage(full_a1,0.9)
ninetypercentcoverage
#Total words in the corpus
totalwordsincorpus<-sum(full_a1$freq)
totalwordsincorpus
plot(x=1:length(dictionary.size.to.coverage.uni),
y=dictionary.size.to.coverage.uni,
type="l", main="Unigram Coverage",
xlab="Dictionary Size (words)",
ylab="Coverage (percent)")
abline (h=50, v= fiftypercentcoverage,col= "blue")
points.default(x=fiftypercentcoverage, y=50, type="p", pch=22, col="black", bg=NA, cex=1.)
abline (h=90, v= ninetypercentcoverage,col= "red")
points.default(x=ninetypercentcoverage, y=90, type="p", pch=22, col="black", bg=NA, cex=1.)
legend(1000, 95, legend=c("90% coverage", "50% coverage"),
col=c("red", "blue"), lty=1:1, cex=0.8)
text(0,50, "50% coverage",pos=4)
text(fiftypercentcoverage,50, as.character(fiftypercentcoverage),pos=1)
text(0,90, "90% coverage",pos=4)
text(ninetypercentcoverage,90, as.character(ninetypercentcoverage),pos=1)
text(1000,20, "Total words in corpus",pos=3,,col = "orange")
text(1000,20, as.character(totalwordsincorpus),pos=1,col = "orange")
install.packages("profvis")
profvis({
data(diamonds, package = "ggplot2")
plot(price ~ carat, data = diamonds)
m <- lm(price ~ carat, data = diamonds)
abline(m, col = "red")
})
library(profvis)
abline(m, col = "red")
plot(price ~ carat, data = diamonds)
m <- lm(price ~ carat, data = diamonds)
library(ggplot2)
library(profvis)
library(ggplot2)
profvis({
data(diamonds, package = "ggplot2")
plot(price ~ carat, data = diamonds)
m <- lm(price ~ carat, data = diamonds)
abline(m, col = "red")
})
source('~/.active-rstudio-document')
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
devtools::install_github("collectivemedia/tictoc")
library(tictoc)
c elapsed
# model fitting: 39.278 sec elapsed
# total: 43.071 sec elapsed
source('~/.active-rstudio-document', echo=TRUE)
shiny::runApp('datascience/capstone/final/Guessword')
runApp('datascience/capstone/JessieJQ/PredictNextWord-master/Shiny App')
runApp()
runApp('datascience/capstone/JessieJQ/PredictNextWord-master/Shiny App')
runApp('datascience/capstone/JessieJQ/PredictNextWord-master/Shiny App')
shiny::runApp('datascience/capstone/Marcella/dataScienceCaptoneProject-gh-pages/guessing_next_word')
runApp('datascience/capstone/Marcella/dataScienceCaptoneProject-gh-pages/guessing_next_word')
runApp('datascience/Projects/shinyapp/app')
library(shinyjs)
runApp('datascience/capstone/Marcella/dataScienceCaptoneProject-gh-pages/guessing_next_word')
runApp('datascience/Projects/shinyapp/app')
runApp('datascience/Projects/shinyapp/app')
runApp('datascience/capstone/Marcella/dataScienceCaptoneProject-gh-pages/guessing_next_word')
runApp('datascience/Projects/shinyapp/app')
runApp('datascience/Projects/shinyapp/app')
runApp('datascience/Projects/shinyapp/app')
runApp('datascience/Projects/shinyapp/app')
runApp('datascience/Projects/shinyapp/app')
runApp('datascience/capstone/Marcella/dataScienceCaptoneProject-gh-pages/guessing_next_word')
runApp('datascience/capstone/Marcella/dataScienceCaptoneProject-gh-pages/guessing_next_word')
runApp('datascience/capstone/Marcella/dataScienceCaptoneProject-gh-pages/guessing_next_word')
runApp('datascience/capstone/Marcella/dataScienceCaptoneProject-gh-pages/guessing_next_word')
runApp('datascience/capstone/Marcella/dataScienceCaptoneProject-gh-pages/guessing_next_word')
runApp('datascience/capstone/Marcella/dataScienceCaptoneProject-gh-pages/guessing_next_word')
runApp('datascience/capstone/Marcella/dataScienceCaptoneProject-gh-pages/guessing_next_word')
arr = array(2:13, dim = c(2, 3, 2))
print(arr)
arr[1]
as.String(arr[1])
runApp('datascience/capstone/Marcella/dataScienceCaptoneProject-gh-pages/guessing_next_word')
runApp('datascience/capstone/Marcella/dataScienceCaptoneProject-gh-pages/guessing_next_word')
runApp('datascience/capstone/Marcella/dataScienceCaptoneProject-gh-pages/guessing_next_word')
runApp('datascience/capstone/Marcella/dataScienceCaptoneProject-gh-pages/guessing_next_word')
runApp('datascience/capstone/Marcella/dataScienceCaptoneProject-gh-pages/guessing_next_word')
runApp('datascience/capstone/Marcella/dataScienceCaptoneProject-gh-pages/guessing_next_word')
runApp('datascience/capstone/Marcella/dataScienceCaptoneProject-gh-pages/guessing_next_word')
shiny::runApp('datascience/capstone/a-submission/shinyapp')
shiny::runApp('datascience/capstone/a-submission/shinyapp v2')
runApp('datascience/capstone/a-submission/shinyapp v2')
runApp()
runApp('datascience/capstone/a-submission/shinyapp v2')
runApp('datascience/capstone/a-submission/shinyapp v2')
runApp('datascience/capstone/a-submission/shinyapp v2')
runApp('datascience/capstone/a-submission/shinyapp v2')
runApp('datascience/capstone/a-submission/shinyapp v2')
runApp('datascience/capstone/a-submission/shinyapp v2')
runApp('datascience/capstone/a-submission/shinyapp v2')
runApp('datascience/capstone/a-submission/shinyapp v2')
runApp('datascience/capstone/a-submission/shinyapp v2')
runApp()
runApp()
runApp('datascience/capstone/a-submission/shinyapp v2')
runApp()
runApp('datascience/capstone/a-submission/shinyapp v2')
runApp()
runApp('datascience/capstone/a-submission/shinyapp v2')
runApp('datascience/capstone/a-submission/shinyapp v2')
runApp('datascience/capstone/a-submission/shinyapp v2')
runApp('datascience/capstone/a-submission/shinyapp v2')
runApp('datascience/capstone/a-submission/shinyapp v2')
runApp('datascience/capstone/a-submission/shinyapp v2')
runApp('datascience/capstone/a-submission/shinyapp v2')
runApp()
runApp('datascience/capstone/a-submission/shinyapp v2')
runApp()
runApp('datascience/capstone/a-submission/shinyapp v2')
runApp('datascience/capstone/a-submission/shinyapp v2')
runApp('datascience/capstone/a-submission/shinyapp v2')
runApp('datascience/capstone/a-submission/shinyapp v2')
runApp('datascience/capstone/a-submission/shinyapp v2')
runApp()
runApp('datascience/capstone/a-submission/shinyapp v2')
runApp('datascience/capstone/a-submission/shinyapp v2')
runApp('datascience/capstone/a-submission/shinyapp v2')
runApp('datascience/capstone/a-submission/shinyapp v2')
runApp('datascience/capstone/a-submission/shinyapp v2')
runApp('datascience/capstone/a-submission/shinyapp v2')
runApp('datascience/capstone/a-submission/shinyapp v2')
runApp('datascience/capstone/a-submission/shinyapp v2')
runApp('datascience/capstone/a-submission/shinyapp v2')
runApp('datascience/capstone/a-submission/shinyapp v2')
runApp('datascience/capstone/a-submission/shinyapp v2')
runApp('datascience/capstone/a-submission/shinyapp v2')
runApp('datascience/capstone/a-submission/shinyapp v2')
runApp('datascience/capstone/a-submission/shinyapp v2')
runApp('datascience/capstone/a-submission/shinyapp v2')
source('~/datascience/capstone/a-submission/shinyapp v2/backoffmethods.R', echo=TRUE)
library(stringr)
#
# This is the server logic of a Shiny web application. You can run the
# application by clicking 'Run App' above.
#
# Find out more about building applications with Shiny here:
#
#    http://shiny.rstudio.com/
# Author: Pradeep Gurav
#
library(stringr)
library(tm)
library(NLP)
source("backoffmethods.R")
bigram <- readRDS("bigram.RData")
trigram <- readRDS("trigram.RData")
quadgram <- readRDS("quadgram.RData")
#use bigram to find next word
find_word_inbigram<-function(split_words) {
next_word<-as.character(head((bigram[bigram$word1==split_words[1],])$word2,4))
freq<-as.character(head((bigram[bigram$word1==split_words[1],])$freq,4))
if(identical(next_word,character(0))) {
next_word<-NA
freq<-0
}
next_word_list<-list(next_word, freq)
return(next_word_list)
}
#use trigram to find next word
find_word_intrigram<-function(split_words) {
next_word<-as.character(head((trigram[trigram$word1==split_words[1] & trigram$word2 == split_words[2],])$word3,4))
freq<-as.character(head((trigram[trigram$word1==split_words[1] & trigram$word2 == split_words[2],])$freq,4))
next_word_list<-list(next_word, freq)
if(identical(next_word,character(0))) {
next_word_list=get_words_with_backoff(split_words[2])
}
return(next_word_list)
}
#use trigram to find next word
find_word_inquadgram<-function(split_words) {
next_word<-as.character(head((quadgram[quadgram$word1==split_words[1]
& quadgram$word2 == split_words[2]
& quadgram$word3 == split_words[3]
,])$word4,4))
freq<-as.integer(head((quadgram[quadgram$word1==split_words[1]
& quadgram$word2 == split_words[2]
& quadgram$word3 == split_words[3]
,])$freq,4))
next_word_list<-list(next_word, freq)
if(identical(next_word,character(0))) {
next_word_list=get_words_with_backoff(paste(split_words[2],split_words[3],sep=" "))
}
return(next_word_list)
}
#backoff logic
get_words_with_backoff<-function(sentence, ngrams_words=0)  {
sentence_c<-stripWhitespace(removeNumbers(tolower(sentence),preserve_intra_word_dashes = TRUE))
split_words<- strsplit(sentence_c," ")[[1]]
qwords<-length(split_words)
#print(split_words)
#print(qwords)
if(ngrams_words==2) { ##use bigram find out next_word
next_word_list<-find_word_inbigram(tail(split_words,1))
}
else if(ngrams_words==3) { ##use trigram find out next_word
next_word_list<-find_word_intrigram(tail(split_words,2))
}
else if(ngrams_words==4) {
next_word_list<-find_word_inquadgram(tail(split_words,3))
}
else {
next_word_list<-list(NA,0)
}
return(next_word_list)
}
runApp('datascience/capstone/a-submission/shinyapp v2')
runApp('datascience/capstone/a-submission/shinyapp v2')
shiny::runApp('datascience/capstone/a-submission/shinyapp v2')
runApp('datascience/capstone/a-submission/shinyapp v2')
slidify('index.Rmd')
library(slidify)
slidify('index.Rmd')
slidify('PG Presentation.Rmd')
setwd("C:/Users/pg/Documents/datascience/capstone/a-submission/presentation")
slidify('PG Presentation.Rmd')
library(markdown)
result <- rpubsUpload(title='PG Datascience Capstone Final presentation',htmlFile='PG Presentation.html',method=getOption('rpubs.upload.method','auto')
result <- rpubsUpload(title='PG Datascience Capstone Final presentation',htmlFile='PG Presentation.html',method=getOption('rpubs.upload.method','auto'))
getwd()
result <- rpubsUpload(title='SPC',contentFile='PG Presentation.html', originalDoc = 'PG Presentation.html')
result <- rpubsUpload(title='PG Datascience Capstone Final presentation',contentFile='PGPresentation.html', originalDoc = 'PGPresentation.html')
result <- rpubsUpload(title='PG Datascience Capstone Final presentation',htmlFile='PGPresentation.html',method=getOption('rpubs.upload.method','auto'))
result <- rpubsUpload(title='PG Datascience Capstone Final presentation',htmlFile='PG Presentation.html',method=getOption('rpubs.upload.method','auto'))
library(RCurl)
result <- rpubsUpload(title='PG Datascience Capstone Final presentation',htmlFile='PG Presentation.html',method=getOption('rpubs.upload.method','auto'))
result <- rpubsUpload(title='PG Datascience Capstone Final presentation',htmlFile='PG Presentation.html',method=getOption('rpubs.upload.method','auto'))
Slide With Plot
========================================================
```{r, echo=FALSE}
plot(cars)
```
